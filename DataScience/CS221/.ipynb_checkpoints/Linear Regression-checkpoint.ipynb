{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechine Learning Definition\n",
    "---\n",
    "_\"Field of study that gives computers the ability to learn without beging expilicty programmed.\"_ <br>~Arthur Samuel (1959)<br>\n",
    "_\"A computer program is said to learn from experience E with respect to some task T and some preformance measure P, if its preformence on T, as measured by P, improves with experiece E.\"_ <br> ~Tom Mitchell (1998)\n",
    "<br>\n",
    "### Supervised Learning\n",
    "Given the _\"right answers\"_ for each example in the data \n",
    "__Regression:__ Predict continuous valued output\n",
    "__Classification:__ Discrete valued output (0 or 1)\n",
    "<br>\n",
    "### Unsupervised Learning \n",
    "here is dataset can you finde some structure ex.(break same data type into two clusters) \n",
    "__Clustering__ used to  Organize Computing clusteres, Social network analysis, Market segmentation, Astronomical data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "---\n",
    "##### Hypothesis:\n",
    "$$h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1 x^{(i)}$$\n",
    "##### Taxonomy:\n",
    "Training Set -> Learning Algorithm -> hypothesis(x) -> output: y\n",
    "##### Notation:\n",
    "m = Number of training examples<br>\n",
    "n = Number of features<br>\n",
    "x´s = \"input\" variable/features<br>\n",
    "y´s = \"output\" variable/feaures<br>\n",
    "$x^{(i)}$ = value x indexed with<br>\n",
    "$x^{(i)}$ = input (features) of $i^{th}$ trainig example<br>\n",
    "$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Cost function\n",
    "---\n",
    "##### Idea:\n",
    "Choose $\\theta_0$,$\\theta_1$ so that $h_\\theta (x)$ is close to $y$ for our traning examples $(x,y)$\n",
    "<br><br>\n",
    "##### Goal: \n",
    "$\\binom{minimize}{\\theta_0,\\theta_1}$ $J(\\theta_0,\\theta_1)$ => Squared error Cost function\n",
    "<br><br>\n",
    "##### Algorithm:\n",
    "$$J(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum\\limits_{i=1}^m(h\\theta_0(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$$h_\\theta(x)$$__\n",
    "(for fixes $\\theta_1$, this is a function of $x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-63f4941d1362>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "points = [[1,1],[2,2],[3,3]]\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "x = np.linspace(0,5,100)\n",
    "plt.plot(x,x*1, \"-r\", label=\"y = 1x\") # best fit\n",
    "plt.plot(x,x*0.5, \"-b\", label=\"y = 0.5x\")\n",
    "plt.plot(x,x*0,\"-y\",label=\"y = 0x\")\n",
    "for lt in points:\n",
    "    for x in lt:\n",
    "        plt.scatter(points[x],points[x])\n",
    "plt.xlabel(\"x\", color=\"r\")\n",
    "plt.ylabel(\"y\", color=\"r\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$$J(\\theta_1)$$__\n",
    "(function of the parameter $\\theta_1$) so define the slope value in this example and slope is calulatet as you know from the school <br>\n",
    "corresponds to value of $\\theta_1 => y = x^2 + 1$ for the minimalization, so the best is 1 since it is the lowest point of parabola \n",
    "<br>$\\binom{minimize}{\\theta_1}$ $J(\\theta_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "---\n",
    "##### Def:\n",
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of __steepest descent__ as __defined__ by the negative of the __gradient__. <br>\n",
    "• Need to choose $\\alpha$ <br>\n",
    "• Needs many iterations <br>\n",
    "• Workd well even when $n$ is large e.g $n = 10^6$\n",
    "<br><br>\n",
    "##### Idea:\n",
    "Init with some $\\theta_0, \\theta_1$ and keep changing $\\theta_0, \\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ till return the minimum.\n",
    "<br><br>\n",
    "##### Goal: \n",
    "Return the $\\binom{min}{\\theta_0,\\theta_1} J(\\theta_0,\\theta_1)$ thus find the __Local Optimum__\n",
    "<br><br>\n",
    "##### Algorithm\n",
    "$\\color{purple}{def}$repeat until convergence{ \n",
    "$$\\theta_j := \\theta_j-\\alpha\\frac{\\delta}{\\delta\\theta_j} J(\\theta_0,\\theta_1)$$\n",
    "<br>\n",
    "$$\\frac{\\delta}{\\delta\\theta_j}\\color{green}{J(\\theta_0,\\theta_1)} = \\frac{\\delta}{\\delta\\theta_j}*\\color{green}{\\frac{1}{2m}\\sum_\\limits{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2}$$\n",
    "<br>\n",
    "$$\\theta_0 := \\theta_0-\\alpha \\color{yellow}{\\frac{1}{m}}\\color{#2180ac}{\\sum_\\limits{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}}) * x^{(i)}$$\n",
    "$$\\color{yellow}{\\frac{\\delta}{\\delta\\theta_1}} \\color{#2180ac}{J(\\theta_0,\\theta_1)} * x^{(i)}$$\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    " } (for j = 0 and j = 1) <=> __Simultaneously__ update $\\theta_0$ and $\\theta_1$ // blue = slope; green = minimalize; $\\alpha :=$ __learning rate__ step sizes<; on yellow you can cross out the $\\delta$; IMPORTANT yellow and blue denote one vector\n",
    "<br>\n",
    "For fixes $\\alpha$ rate: As approach to local minimum, gradient descent steps become smaller automatically, so no need to decrease $\\alpha$ over time\n",
    "<br><br>\n",
    "##### Vectorized the Equation\n",
    "<img src=\"pic/GD Overview.png\">\n",
    "<br><br>\n",
    "##### \"Batch\" \n",
    "Each step of gradient descent uses al lthe training examples, thus the sum of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(2,4),(4,2)]\n",
    "def F(w):\n",
    "   return sum((x * x - y)**2 for x, y in points)\n",
    "def dF(w):\n",
    "   return sum(2*(x*y-y)*x for x, y in points)\n",
    "# Gradient descent\n",
    "w = 0\n",
    "eta = 0.01\n",
    "for t in range(100):\n",
    "   value = F(w)\n",
    "   gradient = dF(w)\n",
    "   w = w -eta * gradient\n",
    "   print(\"iteration {}: w = {}, F(w) = {}\".format(t,w,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "---\n",
    "##### Hypothesis:\n",
    "$$h_\\theta(x) = \\theta_0+\\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n => \\theta^Tx$$\n",
    "<br><br>\n",
    "##### Algorithm:\n",
    "Repeat{$$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_\\limits{i=1}^m(h_\\theta(x^{(i)}-y^{(i)})x_j^{(i)}$$\n",
    "} simultaneously update $\\theta_j$ for j = 0,...,n)\n",
    "<br><br>\n",
    "##### Features Scaling:  \n",
    "##### Mean normalization:\n",
    "Replace $x_i$ with $x_i-\\mu_i$ to make features have approximately zero mean (Do not apply to $x_0 = 1$)<br>\n",
    "$x_1 \\leftarrow \\frac{x_1-\\mu_1}{s_1}$; where $\\mu$ is avg. value of x in traning set; $s$ is ragne, so (max-min)\n",
    "<br>\n",
    "$x_1 = \\frac{x_1-1000}{2000}$<br>\n",
    "$x_2 = \\frac{\\text{x_2} - 2}{5}$<br>\n",
    "$\\therefore -0.5 <= x_1 <= 0.5; -0.5 <= x_2 <= 0.5$<br>\n",
    "Gradient work correctly if after each iter of $\\binom{min}{\\theta}$J($\\theta$) the __value should decrease__ if not you should use lower __learning rate $\\alpha$__;<br>\n",
    "Threshold of iter vary for each case, so Do automatic convergence test function to ensure.<br>\n",
    "For sufficently small $\\alpha$, the $J(\\theta)$ should decrease on each iteration<br>\n",
    "• If $\\alpha$ is too small, then slow convergence<br>\n",
    "• If $\\alpha$ is too large, then $J(\\theta)$ may not decrease on each iternation; may not converge. <br>\n",
    "• Choose $\\alpha$ as $3\\times$ multiplier<br>\n",
    "__Debug:__ ploting that $J(\\theta)$ as the number of Interation.<br>\n",
    "<br>\n",
    "\n",
    "##### Polynomial Regression\n",
    "• Try to combinate the adquate features into one.\n",
    "• Use vary of function in the equation<br>\n",
    "$h_\\theta(x) = \\theta_0+\\theta_1(x)+\\theta(x)^2 $\n",
    "<br>\n",
    "$ h_\\theta(x) =\\theta_0+\\theta_1(x)+\\theta_2\\sqrt{(x)}$ \n",
    "<br>etc.<br>\n",
    "\n",
    "##### Normal Equation _(for non-linear problem, use GD)_\n",
    "Method to sove for $\\theta$ analytically. <br>\n",
    "• No need to choose $\\alpha$ <br>\n",
    "• Do not need to iterat <br>   \n",
    "• Slow if $n$ is very large e.g after n=10000 <br>\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$<br>\n",
    "$m$ __examples__ $(x^{1},y^{1},...,(x^{(m)},y^{m})$ <br>while $n$ __features__\n",
    "where: <br>X is $m\\times(n+1)$ matrix, so X =\\begin{bmatrix}\n",
    "-& (x^{1})^T &-\\\\\n",
    "-& (x^{2})^T &-\\\\\n",
    "-& (x^{m})^T &-\\\\\n",
    "\\end{bmatrix} <br>\n",
    "<br> If $X^T X$ is non-invertible, then caused by:<br>\n",
    "• Redundant features (linearly dependent)\n",
    "    - e.g. x_1 = size in feet^2\n",
    "           x_2 = size in m^2\n",
    "• Too many features (e.g. $m <= n$)\n",
    "    - Deltete some features, or use regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
