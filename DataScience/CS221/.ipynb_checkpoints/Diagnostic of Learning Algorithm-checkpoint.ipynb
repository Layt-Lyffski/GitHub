{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (see the R Overview on GitHub @APL-Overviews repo.)\n",
    "<br><br>\n",
    "# Advice for Applying ML\n",
    "---\n",
    "##### Usually:\n",
    "- Get more training examples -> fixes high variance\n",
    "- Compute smaller sets of features -> fixes high variance\n",
    "- Getting additional features -> fixes high bias\n",
    "- Adding polynomial features ($x_1^2,x_2^2, x_1 x_2, etc$) -> fixes high bias\n",
    "- Decreasing $\\lambda$ -> fixes high bias<br>\n",
    "- Increasing $\\lambda$ -> fixes high variance\n",
    "\n",
    "\n",
    "### Machine Learning diagnostic:\n",
    "Diagnostic: A test to run to gain insight what is/is not working with learning algortihm, and gain guidance as to how best to improve its performence.\n",
    "Although Diagnositc can take long time to implement, yet worth it, since it is very good use of your time, as instead using the above adviced, thus __Preform Diagnostic__ <br><br>\n",
    "\n",
    "#### Training/testing procedure fo linear regression\n",
    "- Learn parameter $\\theta$ form training data (minimizing training error $J(\\theta)$)\n",
    "- Compute test set error: (see the min function on other files; e.g SSE)\n",
    "- Mislassification error (0/1 misclassifiaciotn error): BELOW: red = error; green = !error\n",
    "$$\\bigg\\{ \\frac{\\color{red}{1 \\rightarrow \\text{ if } h_\\theta>=0.5, y = 0; ||\\text{ if } h_\\theta(x)<0.5, y=1;}}{\\color{green}{0 \\rightarrow otherwise}}$$\n",
    " $$\\text{Test error } = \\frac{1}{m_{test}} \\sum_\\limits{i=1}^{m_{test}} \\text{err}(h_\\theta(x_{test}^{(i)}), y_{test}^{(i)})$$\n",
    "\n",
    "#### Overfitting desc\n",
    "Once parameters $\\theta_0, \\theta_1,...,\\theta_n$ been fit to some set of data (usually training set), the error of the parmeters as measure on that data (the training error $J(\\theta)$ is likely to be lower than the acutal generalization error.\n",
    "\n",
    "#### Model selction (see R overview)\n",
    "__Problem:__ $J_{test}(\\theta^{(5)})$ is likey to be an optimistic estimate of generalization error. i.e. our extra parameter ($d$ = degree of polynomial) is fit to test set.\n",
    "<br><br><br>\n",
    "\n",
    "# Diagnosing bias vs variance\n",
    "---\n",
    "Suppose your learning algorihm is preforming less well then you were hoping.\n",
    "($J_{cv} (\\theta)$ of $J_{test}(\\theta)$ is high)<br> \n",
    "__Causes by:__ <br>\n",
    "##### Bias (underfit):\n",
    "$J_{\\text{train}}(\\theta)$ will be high $\\therefore J_{cv}(\\theta) \\approx J_{\\text{train}}(\\theta)$<br><br>\n",
    "##### Variance (overfit):\n",
    "$J_{\\text{train}}(\\theta)$ will be low $\\therefore J_{cv}(\\theta) >> J_{train}(\\theta)$\n",
    "<br>\n",
    "<img src=\"pic\\bias_vs_variance.png\">\n",
    "\n",
    "## Regularization\n",
    "Large $\\lambda \\rightarrow$ high bias (underfit); Intermediate $\\lambda$ \"just right\"; Small $\\lambda \\rightarrow$ high variacne (overfit)\n",
    "<br><br>\n",
    "\n",
    "#### Choosing the regularization parameter  $\\lambda$\n",
    "__Model:__\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_4 x^4$$\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_\\limits{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_\\limits{j=1}^m \\theta_j^2$$\n",
    "<br>\n",
    "__Loop:__ (step of 0.3 multiplier, work as well)<br>\n",
    "__1. Try__ $\\lambda = 0.00$ $\\rightarrow \\binom{min}{\\theta} \\rightarrow \\theta^{(1)} \\rightarrow J_{cv}(\\theta^{(1)})$<br>\n",
    "__2. Try__ $\\lambda = 0.01$ $\\rightarrow \\binom{min}{\\theta} \\rightarrow \\theta^{(2)} \\rightarrow J_{cv}(\\theta^{(2)})$<br>\n",
    "__3. Try__ $\\lambda = 0.02$<br> \n",
    "__4. Try__ $\\lambda = 0.04$ $\\rightarrow ... \\rightarrow ... \\rightarrow J_{cv}\\theta^{(5)}$<br>\n",
    "__...__<br>\n",
    "__12. Try__ $\\lambda = 10.$ $\\rightarrow...\\rightarrow \\theta^{(12)} \\rightarrow J_{cv}(\\theta^{(12)})$<br><br>\n",
    "Pick (say) $\\theta^{(5)} \\therefore$ Test error: $J_{test}(\\theta^{(5)})$\n",
    "<br><br><br>\n",
    "\n",
    "#### Bias/Variacne as a function for the regularization parameter $\\lambda$\n",
    "<img src=\"pic\\fu_of_bais_variance.png\">\n",
    "<br>\n",
    "\n",
    "##### Learning curves\n",
    "__High bias:__\n",
    "    - If a learning algorithm is suffering from high bias, getting more training data __WON'T__ (by itself) help much<br>\n",
    "__High Variance:__ the differece between $J_{cv}(\\theta)$ and $J_{train}(\\theta)$\n",
    "    - If a learning algorithm is suffering form high variacne, getting more training data is likely to help\n",
    "__DO PLOT THE LEARNING CURVES IT REALLY HELP__ \n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "# Machine Learning System Design\n",
    "---\n",
    "### A way of Approach\n",
    "    - Start with a simple algorithm that you can implement quickly. \n",
    "        Implement and test it on the cross-validation data; \n",
    "            after you can distribute your focus VERY GOOD => DO IT IMPROTANT\n",
    "    - Plot learning curves to decide wheater you need more data or more freatures, etc. \n",
    "        It will likely help => Pre-Mature-Optimizaiton\n",
    "    - Error analysis: Manually examine the examples (in corss validation set) that \n",
    "        your algorithm made errors on. See if you spot any systematic trend\n",
    "        in what type of examples it is making errors on. \n",
    "##### Error analysis:\n",
    "__THUS__, just indeed do a __Analysis__; divide each error, count them, to see where the errors occure most frequently, and focus your time there (see importance of numerical evaluation => e.g. software \"stemming\" but do consider before wheater it will not cause problem or upper or lower case)\n",
    "<br><br>\n",
    "#### See R Overview @APL-Overviews repo. \n",
    "for error Metriics for skewed classes and things like presicion/recalls\n",
    "<br>\n",
    "<img src=\"pic\\precision_recall.png\">\n",
    "<br><br>\n",
    "\n",
    "## Trading off Precision and Recall \n",
    "__Def:__ to vary the Threshold of classification, wheater 1 or 0\n",
    "<img src=\"pic\\tradingoff_p_r.png\">\n",
    "<br>\n",
    "Where: $F_1$ Score: $2\\frac{PR}{P+R}$\n",
    "<br><br><br>\n",
    "## Large Data rationale\n",
    "Use a learning algorithm with many parameters (e.g. logistic regression/linear regression with many features; neutral network with many hidden units) <=> low bias Algorithm<br>\n",
    "$\\therefore J_{train}(\\theta)$ will be small\n",
    "<br><br>\n",
    "Use a very large traininig set (unlikely to overfit) <=> low variance<br>\n",
    "$\\therefore J_{train}(\\theta) \\approx J_{test}(\\theta)$<br><br>\n",
    "Putting it all together, provide low bias and low variance <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
