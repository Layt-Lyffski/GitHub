{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Componet Analysis (PCA)\n",
    "---\n",
    "prediciton error the distace between point and where it get prdection on line/area\n",
    "##### Reduce from 2-dimension to 1-dimension: \n",
    "Find a direction (= vector $u^{(1)}\\in\\mathbb{R}^n$) onto which to project the data so to minimize the projection error.\n",
    "##### Redcue from n-dimension to k-diemension:\n",
    "Find $k$ vectors $u^{(1)},u^{(2)},...,u^{(k)}$ onto which to project the data, so as to minimize the projection error.<br>\n",
    "__THUS__ try to finde the lowers distance between point and predictive slope\n",
    "<br>\n",
    "__NOTE:_ PCA is not linear regression\n",
    "$\\because$ in linear regression you calcualte the vertical distance between the slope and point, where in PCA you calculate the, indeed, shortest distance between the slope and point (so basically __orthogonal__ to slope) and PCA is not supervised learning, so no search for $y$.\n",
    "<br><br>\n",
    "\n",
    "### Data Preprocessing\n",
    "Trainign set: $x^{(1)},x^{(2)},...,x^{(Â´m)}$<br>\n",
    "Preprocessing (= feature scaling/mean normalization -> do always):<br>\n",
    "$\\mu_j = \\frac{1}{m}\\sum_\\limits{i=1}^m x_j^{(i)}$ => the mean of each feature<br>\n",
    "then repace each $x_j^{(i)}$ with $x_j -\\mu_j$<br>\n",
    "(then you can also scale the features; see below)\n",
    "<br>\n",
    "If different features on differetn scales (e.g. $x_1 =$ size of house, $x_2 =$ number of bedrooms), scale features to have comparable range of values. (SEE SCALLING ON LINEAR REGRESSION)<br>\n",
    "$x_j^{(i)} \\leftarrow \\frac{x_j^{(i)}-\\mu_j}{s_j}$, MEAN substract the mean, and dived by measuer of the range of values of features j, so it can be wheater the (max - min)j value or the standard deviation<br>\n",
    "__NOTE__ the Mathematical proof of the best \"slope\" is very high mathematic, so here is not included<br><br>\n",
    "\n",
    "## Algorithm of PCA\n",
    "reduce data from $n$-dimensions to $k$-dimensions Compute \"covariacne matrix\":<br>\n",
    "\n",
    "(just a matrix notation) $\\Sigma = \\frac{1}{m}\\sum_\\limits{i=1}^n\\color{red}{(x^{(i)})(x^{(i)})^T}$<br>\n",
    "\n",
    "**(where $x^{(i)}$ is $n\\times1$ AND $(x^{(i)})^T$ is $1\\times n \\therefore$ IT IS $n\\times n$)<br>\n",
    "\n",
    "Compute \"eigenvectors\" of matric $\\Sigma$ (it is matrix): where result also is $n\\times n$ matrix. <br>\n",
    "__THUS__ the first k-vetors of $\\Sigma_{reduced}$-Matirx build the redueced version of the $\\Sigma$ matrix which originaly had higher dims\n",
    "##### End example:\n",
    "$x\\in\\mathbb{R}^n --> z\\in\\mathbb{R}^k$<br>\n",
    "\n",
    "$z^{(i)}= \n",
    "\\begin{bmatrix}\n",
    "|&|&|&|\\\\\n",
    "u^{(1)} & u^{(2)} & ... & u^{(k)}\\\\\n",
    "|&|&|&|\\\\\n",
    "\\end{bmatrix}^T * x^{(i)}$ <br>\n",
    "\n",
    "where is is $n\\times k$<br>\n",
    "\n",
    "<!-- $x^{(i)}$ = $\n",
    "\\begin{bmatrix}\n",
    "- & u^{(1)}^T & - \\\\\n",
    "- & w & - \\\\\n",
    "- & w & - \\\\\n",
    "- & u^{(k)}^T & - \\\\ \n",
    "\\end{bmatrix}$ --><br>\n",
    "(where is is $k\\times n$) * $x^{(i)} \\text{ is a } n \\times 1$<br>\n",
    "__THUS__ $x^{(i)}$ is a $k\\times 1$<br>\n",
    "\n",
    "##### Summary\n",
    "After mean normalization (ensure every features has zero mean) and optionally fearues scaling:\n",
    "implement := Sigma = (1/m) * X^T * X\n",
    "<br><br>\n",
    "\n",
    "### Choosing the Number of Principal Components\n",
    "__choosing $k$ (number of principal components)<br>\n",
    "Average squared projection error: $\\frac{1}{m}\\sum_\\limits{i=1}^m ||x^{(i)}-x_{approx}{(i)}||^2$<br>\n",
    "Total variation in the data: $\\frac{1}{m}\\sum_\\limits{i=1}^m ||x^{(i)}||^2$<br>\n",
    "\n",
    "Typically, choose $k$ to be smallest value so that\n",
    "$$\\frac{\\frac{1}{m}\\sum_\\limits{i=1}^m ||x^{(i)}-x_{approx}{(i)}||^2} {\\frac{1}{m}\\sum_\\limits{i=1}^m ||x^{(i)}||^2} <= 0.01 (= 1\\%) \\therefore  99\\% \\text{ of variancei s retained}$$\n",
    "#### Algorithm for $k$\n",
    "<img src=\"pic\\pca_k_algo.png\">\n",
    "<br>\n",
    "\n",
    "##### Summary\n",
    "\n",
    "[U,S,V] = svd(Sigma) or eig(Sigma)jj // in Octave PL<br>\n",
    "pick smallest value of $k$ for which (i.e. k=100)<br>\n",
    "$\\frac{\\sum_\\limits{i=1}^k S_{ii}}{\\sum_\\limits{i=1}^m S_{ii}} >= 0.99 \\therefore$ 99% of variance retained\n",
    "<br><br>\n",
    "\n",
    "### Reconstruction form compressed representation\n",
    "<img src=\"pic\\dim_reduction_ reconstruction.png\">\n",
    "<br>\n",
    "\n",
    "## Advice for Applying PCA\n",
    "__DO NOT USE PCA__ to prevent overfitting instead just __USE REGULARIZATION__<br>\n",
    "##### Supervised learning speedup \n",
    "\n",
    "$(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})$ <br>\n",
    "__Extract inputs:__<br>\n",
    "Unlabeled dataset:\n",
    "$$ x^{(1)},x^{(2)},...,x^{(m)} \\in \\mathbb{R}^{10000} \\leftarrow x$$\n",
    "$$\\downarrow PCA \\downarrow$$\n",
    "$$ z^{(1)},z^{(2)},...,z^{(m)} \\in \\mathbb{R}^{1000} \\leftarrow z \\therefore \\text{ (i.e. } h_\\theta(z))$$<br>\n",
    "New training set: $(z^{(1)}, y^{(1)}),(z^{(2)}, y^{(2)}),...,(z^{(m)}, y^{(m)})$ <br>\n",
    "<br>\n",
    "##### NOTE\n",
    "Mapping (= $U_{reduce})$ $x^{(i)} \\rightarrow z^{(i)}$ should be defined by running PCA only on the training set. This mapping can be applied as well to the examples $x_{cv}^{(i)}$ and $x_{test}^{(i)}$ in the cross calidatiaon and test sets.<br>\n",
    "<br>\n",
    "##### Summary of Application of PCA:\n",
    "- Compression\n",
    "    - Reduce memory/disk needed to store data\n",
    "    - Speed up learning algorithm\n",
    "        - Both mean: Choose $k$ by % of variance retain\n",
    "- Visualizaiton \n",
    "    - k = 2 or k = 3.\n",
    "<br><br>\n",
    "\n",
    "##### Way of Aproach\n",
    "Before implementing PCA, first try running whatever you want to do with the original/raw data $x^{(i)}$. Only if that does not do what you want, then implement PCA and consider using $z^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
