{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "##### Sigmoid/Logcistic function\n",
    "$h_\\theta(x) = g(\\theta^T x) \\rightarrow g(z) = \\frac{1}{1+e^{-z}}\\rightarrow h_\\theta(x) = \\frac{1}{1+e^{-\\theta^T x}}$<br>\n",
    "\n",
    "$\\therefore h_\\theta(x) = P(y=1|x;\\theta) \\therefore$ $0 <= h_\\theta(x) <= 1 $<br>\n",
    "\n",
    "$\\therefore$ probability that $y = 1$, given $x$, parameterized by $\\theta$\n",
    "<br>\n",
    "<br>\n",
    "### Decison Boundary\n",
    "##### Linear Boundary\n",
    "$h_\\theta(x) = g(\\theta_0+\\theta_1 x_1 + \\theta_2 x_2)$\n",
    "\n",
    "Predict $y = 1$ if $-2+x_1+x_1 >= 0$ \n",
    "// you can vary with the predict by combinating with signs e.g $0+x_1+x_2 = 3 \\rightarrow y =0$\n",
    "<br><br>\n",
    "\n",
    "\n",
    "##### Non-linear Boundary\n",
    "$h_\\theta(x)=g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 +\\theta_3 x1_1m^2 + \\theta_4 x_2^2)$\n",
    "\n",
    "<br>\n",
    "Predict $y = 1$ if $-1 +x_1^2 + x_2^2 >= 0$<br>\n",
    "\n",
    "### Cost function\n",
    "\n",
    "$$Cost_(h_\\theta(x),y)= \\binom{-log(h_\\theta)) if y = 1}{-log(1-h_\\theta(x)) if y = 0}$$\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_\\limits{i=1}^m Cost(h_\\theta(x^{(i)}), y^{(i)})$$\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m} \\sum_\\limits{i=1}^m \\color{green}{y^{(i)} log h_\\theta(x^{(i)})}+\\color{red}{(1-y^{(i)}) log (1-h_\\theta(x)^{(i)}))}$$\n",
    "\n",
    "Fit parametrs $\\theta: \\binom{min}{\\theta} J(\\theta)$<br>\n",
    "Prediction given new $x$:\n",
    "Output $h_\\theta(x) = \\frac{1}{1+e^{\\theta x}}$<br>\n",
    "$\\therefore$ want $min_\\theta J(\\theta)$<br>\n",
    "\n",
    "Repeat { <br>\n",
    "$$\\theta_j := \\theta_j - \\alpha \\color{#2080ac}{\\frac{\\delta}{\\delta \\theta_j}J(\\theta)}$$\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\color{#2080ac}{ \\sum_\\limits{i=1}^m (h_\\theta(x^{(i)}), y^{(i)}) x_j^{(i)}}$$\n",
    "\n",
    "} (simultaneously update all $\\theta_j$) //NOTE: altough the Algorihm loook id. with liear Regression, DENOTE: that the $h_\\theta$ (Hypothesis) change<br>\n",
    "\n",
    "__Converging__ work the same as in Linear Regression\n",
    "<br>\n",
    "#### Advanced Optimizaiton not need for learning rate alpha\n",
    "• Conjugate gradient<br>\n",
    "• BFGS<br>\n",
    "• L-BFGS<br>\n",
    "\n",
    "## Mulitclass Classification\n",
    "##### One-vs-all/rest\n",
    "$$h_\\theta^{(i)}(x) = P(y=i|x;\\theta) WHERE (i=1,2,3)$$\n",
    "\n",
    "Train a logistic regression classifire $h_\\theta^{(i)}$ for each class $i$ to predict the probability that $y = i$<br>\n",
    "\n",
    "On a new input $x$, to make a prediction, pic the class $i$ that maximizes $\\binom{max}{i}h_\\theta^{(i)}(x)$\n",
    "\n",
    "<images src=\"pic/one-vs-all.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine\n",
    "#### Form:\n",
    "$$\\binom{min}{\\theta}C\\sum_\\limits{i=1}^m [ y^{(i)}\\text{ cost}_1(\\theta^Tx^{(i)})+(1-y^{(i)} \\text{ cost}_0(\\theta^Tx^{(i)} ] + \\frac{1}{2}\\sum_\\limits{i=1}^n \\theta_j^2$$\n",
    "##### Hypothesis:\n",
    "$h_\\theta(x) = 1 \\text{ if } \\theta^Tx >= 0 : 0 \\text{ otherwise}$\n",
    "<img src=\"pic\\svm.png\"><br>\n",
    "\n",
    "## SVM Decision boundary\n",
    "$$\\binom{min}{\\theta}C\\sum_\\limits{i=1}^m \\color{#2080ac}{([ y^{(i)}\\text{ cost}_1(\\theta^Tx^{(i)})+(1-y^{(i)} \\text{ cost}_0(\\theta^Tx^{(i)} ]=0)} + \\color{cyan}{\\frac{1}{2}\\sum_\\limits{i=1}^n \\theta_j^2 == \\frac{1}{2}||\\theta||^2}$$\n",
    "$$\\text{min } C\\times \\color{#2080ac}{0} + \\frac{1}{2}\\sum_\\limits{i=1}^n \\theta_j^2$$\n",
    "$$\\therefore$$\n",
    "$$\\theta^Tx^{(i)} >= 1 \\text{ if } y^{(i)} = 1 \\text{ OR } \\theta^Tx{(i)} <= -1 \\text{ if } y^{(i)} = 0$$\n",
    "<br>\n",
    "<img src=\"pic\\svm_db.png\">\n",
    "<br><br>\n",
    "## Kernels\n",
    "##### Notation:\n",
    "$l^{(i)}$ = landmarks of i<br>\n",
    "$f_1$ feature creted \"due to Kernerls usej\"<br>\n",
    "$||x||$ = lengt of x on graph; \"where x is regular data feature point\"<br>\n",
    "\n",
    "### Kernels and Similarity (Guassion kernerls)\n",
    "$f_1 = \\text{ similarity } (x,l^{(1)} = exp(-\\frac{\\color{red}{||x-l^{(i)}||^2}}{2 \\sigma^2}) = exp(-\\frac{\\sum_\\limits{j=1}^n(x_j-l_j^{(1)})^2}{2\\sigma^2})$<br>\n",
    "\n",
    "If $\\color{red}{x \\approx l^{(1)}}:0 :=> f_1 \\approx exp(-\\frac{0^2}{2\\sigma^2})\\approx 1$<br>\n",
    "\n",
    "If $x$ is far from $l^{(1)} :=> f_1$ = exp(-$\\frac{\\text{(large number)}^2}{2\\sigma^2}) \\approx 0$<br>\n",
    "\n",
    "__THUS,__ for example you have $(l{(1)},l{(2)},l{(3)})$ and you have one given $x$ then you can combine those and of one $x$ 3 differet features like: ($f_1, f_2, f_3$)\n",
    "<br><br>\n",
    "\n",
    "#### Example:\n",
    "<img src=\"pic\\kernels_ex.png\"><br>\n",
    "<br><br>\n",
    "\n",
    "### SVM with Kernels\n",
    "Given $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$<br>\n",
    "choose $l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},...,l^{(m)}=x^{(m)}$<br>\n",
    "Given exmaple $x$:<br>\n",
    "$f_i = \\text{ similarity }(x, l^{(i)}; \\text{where } l^{(i)} = x^{(i)}$ etc..<br>\n",
    "##### Example\n",
    "for Training exmpale $(x^{i},y^{(i)}):$<br>\n",
    "$$x^{(i)} \\rightarrow f^{(i)} =\n",
    "\\begin{equation}\n",
    "   \\begin{bmatrix} \n",
    "    f_1^{(i)} = sin(x^{(i)}, l^{(i)}) \\\\\n",
    "    f_1^{(i)} = sin(x^{(i)}, l^{(i)}) \\\\\n",
    "    ... \\\\\n",
    "    \\color{red}{f_i^{(i)} = sin(x^{(i)}, (l^{(i)}=x^{(i)})) = exp (-\\frac{0}{2\\sigma^2}) = 1}\\\\\n",
    "    \\text{ is is landmark exactly on the point x} \\\\\n",
    "    ... \\\\\n",
    "    f_1^{(i)} = sin(x^{(i)}, l^{(i)})\n",
    "   \\end{bmatrix} \n",
    "\\end{equation}$$\n",
    "<br><br>\n",
    "\n",
    "### Hypothesis\n",
    "Given $x$, compute features $f \\in \\mathbb{R}^{m+1}$<br>\n",
    "Predict \"y=1\" if $\\theta^T f >= 0$; where $\\theta^T f = \\theta_0 f_0 + \\theta_1 f_1 +...+ \\theta_m f_m \\therefore \\theta \\in \\mathbb{R}^{m+1}$ \n",
    "\n",
    "##### Training:\n",
    "$$\\binom{min}{\\theta} C \\sum_\\limits{i=1}^m y^{(i)} cost\\color{red}{(\\theta^T f^{(i)})} + (1-y^{(i)} = \\text{cost}_0 \\color{red}{(\\theta^T f^{(i)})} + \\color{green}{\\frac{1}{2}\\sum_\\limits{j=1}^{n=m}\\theta_j^2}$$\n",
    "\n",
    "$\\color{red}{(\\theta^T f^{(i)})}$ just change from $\\color{red}{(\\theta^Tx^{(i)})}$ due to the __Kernels use__<br>\n",
    "\n",
    "Green <=> $\\sum_\\limits{j}\\theta_j^2 = \\theta^T M \\theta$; where M is a scaler, so the machine do the same as in formel, yet they on rescaled version. <br>\n",
    "\n",
    "__THUS__ it allow to scalre to much bigger datasets and optimized, so it is done for Computation efficiently.\n",
    "<br><br>\n",
    "\n",
    "#### SVM Parameters\n",
    "$\\color{#2080ac}{C(=\\frac{1}{\\lambda})}$<br>\n",
    "Large C: Lower bias, high variance. $\\rightarrow$ (small $\\lambda$)<br>\n",
    "Small C: Higher bias, low variance. $\\rightarrow$ (large $\\lambda$)<br>\n",
    "##### Linear kernel\n",
    "It is acutaly no kernal $\\rightarrow$ prdict \"y=1\" if $\\theta^Tx >= 0$; Use when $n$ large, $m$ small ... $x\\in\\mathbb{R}^{n+1}$<br>\n",
    "##### Gaussian kernel\n",
    "$$f_i = exp(-\\frac{||x-l^{(i)}||^2}{2\\sigma^2}); \\text{where } l^{(i)}=x{(i)}$$\n",
    "need to choose $\\sigma^2$ use when $x \\in \\mathbb{R}$; $n$ small; and/or $m$ large <br>\n",
    "__NOTE:__ do prefore feature scaling before using the Gaussian kernel.\n",
    "\n",
    "$\\color{#2080ac}{\\sigma^2}$<br>\n",
    "Large $\\sigma^2$: Features $f_i$ vary more smoothly (see graph) $\\therefore$ Higher bias, lower variance.<br>\n",
    "Small $\\sigma^2$: Features $f_i$ vary less smoothly (see graph) $\\therefore$ Lower bias, higher vairance.\n",
    "<br><br>\n",
    "### Other choices of kernel\n",
    "__NOTE:__<br> \n",
    "Not each similaarty fucntions $\\text{similarity} (x,l)$ are valid kernels.<br>\n",
    "(Need to satisfy technical condition called \"Mercer's Theorem\" to make sure SVM packages optimizations rund correctly, and do not diverage).\n",
    "<br><br>\n",
    "##### Other Kernels:\n",
    "Polynomial kernel: $(x^T l + \\text{constant})^{\\text{degree}}$<br>\n",
    "String kernel, chi-square kernel, histogram intersection kernel;\n",
    "\n",
    "### Multi-class classification\n",
    "Many SVM packages already have build-in multi-class classification functionality.<br>\n",
    "Otherwise, use one-vs-all method. (Train K SVMs, one to distinguish $y=i$ from the rest,<br>\n",
    "for $i = 1,2,...,K)$, get $\\theta^{(1)}, \\theta^{(2)},...,\\theta^{(K)}$, where $\\theta^{(i)} = y = i$<br>\n",
    "Pick class $i$ with largest $(\\theta^{(i)})^Tx$\n",
    "\n",
    "## Logistic regression vs. SVMs\n",
    "$n$ = number of features ($x \\in \\mathbb{R}^{n+1}$), m = number of training examples<br>\n",
    "if $n$ is large (relative to $m$): (e.g: n>=m, n=10000, m=10...0000)<br>\n",
    "$\\therefore$ Use logistic regression, or SVM withou a kernel (\"linear kernel\")<br>\n",
    "<br>\n",
    "if $n$ is small, $m$ is intermediate: (e.g: n=1-1000, m=10-10000)\n",
    "$\\therefore$ Use SVM with Gaussian kernel<br>\n",
    "<br>\n",
    "if $n$ is small, $m$ is large: (n=1-1000, m=50000+)\n",
    "$\\therefore$ create/add more features, then use logistic regression or SVM without kernel<br>\n",
    "<br>\n",
    "Neutral netwrok likely to work well for most of these settings, but may be slower to train.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
