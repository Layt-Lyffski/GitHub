{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "## Addressing Overfitting\n",
    "##### Options:\n",
    "1. Reduce number of features.\n",
    "    - Manually select which features to keep.\n",
    "    - Model seleciton algorithm $\\rightarrow$ choosing the right $\\lambda$ (=Regularization parameters)\n",
    "2. Regularization\n",
    "    - Keep all the features, but reduce magnitude/valose of parameters $\\theta_j$.\n",
    "    - Works well when we have a lot of features, each of which contributes a bit to predictiong $y$.\n",
    "__THUS__ Overfit $\\rightarrow$ Fails to generalize to new examples not in training set.\n",
    "    \n",
    "## Regularization\n",
    "Small values for parameters ($\\theta_0,\\theta_1,...,\\theta_n)$ e.g. $\\color{pink}{\\rightarrow \\theta_3 and \\theta_4 \\approx 0}$ // do not affect on $\\theta_0$\n",
    "$\\color{pink}{\\textrm{Regularization Term }}$, where is $\\color{pink}{\\lambda}$ Regularization param.\n",
    "    - Simpler hypothesis \n",
    "    - Less prone to overfitting\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_\\limits{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2 \\color{pink}{ + \\lambda\\sum_\\limits{j=1}^n \\theta_j^2}$$\n",
    "\n",
    "#### Underfit\n",
    "occure, also when the $\\lambda$ if to high value, is so the Regression become to much simplify ($\\rightarrow$horziontal line)<br> $\\because$ $\\theta_1, \\theta_2,...,\\theta_n \\approx 0$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Algortithm\n",
    "##### Gradient Descent\n",
    "Reapt{\n",
    "$$\\theta_0:=\\theta_0-\\alpha \\frac{1}{m}\\color{green}{ [ } \\sum_\\limits{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\color{green}{ ] } \\rightarrow \\color{green}{\\frac{\\delta}{\\delta \\theta_0}J(\\theta)}$$\n",
    "\n",
    "$$\\theta_j=:\\color{red}{\\theta_j}-\\color{red}{\\alpha} \\color{cyan}{[\\frac{1}{m}\\sum_\\limits{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\color{pink}{-\\frac{\\lambda}{m}\\theta_j}]}$$\n",
    "\n",
    "} where j > 0 && $\\color{red}{1-\\alpha\\frac{\\lambda}{m}}$$$\\theta_j:=\\color{pink}{[\\color{red}{\\theta_j(1}-\\color{red}{\\alpha}\\frac{\\lambda}{m})]} - \\color{cyan}{\\alpha\\frac{1}{m}\\sum_\\limits{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}}$$<br>\n",
    "<br>\n",
    "\n",
    "## Linear Fitting\n",
    "If we have too many __features__, the learned hypothesis may fit the training set very well, but fail to __generalize__ to new examples\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_\\limits{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2 \\approx 0$$\n",
    "\n",
    "<br>\n",
    "\n",
    "## Logistic Fitting\n",
    "<img src=\"pic\\overfit.png\">\n",
    "\n",
    "## Alternative view of Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
