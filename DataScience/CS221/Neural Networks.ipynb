{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral Networks\n",
    "---\n",
    "Algorithms that try to mimic the brain.<br>\n",
    "## Representation\n",
    "---\n",
    "##### Notation\n",
    "$a_i^{(j)}$ = \"activation\" of unit $i$ in layer $j$<br>\n",
    "$\\Theta^{(j)}$ = matrix of weigths/parameters controlling fucniton mapping formm layer $j$ to layer $j+$<br>\n",
    "L = total no. of layers in network <br>\n",
    "$s_l$ = no. of units (not counting bias unit) in layer $l$<br>\n",
    "\n",
    "If network has $s_j$ units in layer $j$, $s_j+1$ units in layer $j+1$, then$\\Theta^{(j)}$ will be of demeinsion $s_j+1\\times(s_j +1)$<br>\n",
    "__Thus__ neutral network is __compute succesfully__<br><br>\n",
    "\n",
    "##### Visualization by Andrew Ng\n",
    "<img src=\"pic\\forward_propagation.png\"><br>\n",
    "Given one training example(x,y): where $a^{(1)}$ input layer; rest hidden; $a^{(4)}$ output layer (Mulitclass)\n",
    "<img Src=\"pic\\1ex_forward_propagation.png\"><br>\n",
    "##### Intuition\n",
    "<img src=\"pic\\xnor_nn.png\"><br>\n",
    "<img src=\"pic\\graphic_forward_propagation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "---\n",
    "#### Binary classifaction\n",
    "$y$ = 0 or 1<br>\n",
    "1 output unit<br>\n",
    "\n",
    "#### Mulit-class classification (K classes)\n",
    "$y from R^K$<br>\n",
    "K output units<br>\n",
    "### Cost function\n",
    "$h_\\theta(x)\\in\\mathbb{R}^K (h_\\theta(x))_i = i^th$ output; where pink is __Regularization__ and is sum over __$\\Theta_ji^{(l)}$__ without Regulaizred the __Bais__ && green taking the $h$ and compare it to the $y$ which tell the right __\"classified\"__ result; CYAN only if multiply outputs is summation over the output units\n",
    "\n",
    "$$J(\\Theta)=-\\frac{1}{m} [ \\sum_\\limits{i=1}^m \\color{cyan}{\\sum_\\limits{k=1}^K} \\color{green}{y_k^{(i)}} log\\color{green}{(h_\\theta(x^{(i)}))_k} + (1-y_k^{(i)}) log(1-(h_\\theta(x^{(i)}))_k) ] + \\color{pink}{\\frac{\\lambda}{2m}\\sum_\\limits{l=1}^{L-1} \\sum_\\limits{i=1}^{s_1} \\sum_\\limits{j=1}^{s_l+1}(\\Theta_ji^{(j)})^2}$$\n",
    "\n",
    "$$cost(i) = y^{(i)}log h_\\theta(x^{(i)})+(1-y^{(i)}log h_\\theta(x^{(i)})$$\n",
    "$$\\text{Only for Intition: } cost(i) \\approx (h_\\theta(x^{(i)})-y^{(i)})^2$$<br> \n",
    "__THUS__ the difference between the real value and what the algorihm output was\n",
    "<br>\n",
    "<br>\n",
    "### Backpropagation Algorithm\n",
    "__Intuition:__ $\\delta_j^{(l)}$ = \"error\" of node $j$ in layer $l$<br>\n",
    "__Goal:__ compute the derivatives<br>\n",
    "__Ex:__ $\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta) = a_j^{(l)} \\delta_i^{(l+1)}$ (ignoring $\\lambda$; if $\\lambda = 0$ __THUS__ the Regularization goes away'<br><br>\n",
    "__Info:__ bais always equal to +1\n",
    "##### Algorithm\n",
    "Training set {$(x^{(1)}, y^{(1)}),...,(x{(m)}, y^{(m)})$}<br>\n",
    "Set $\\Delta_{ij}^{l} = 0$ (for all l,i,j)\n",
    "\n",
    "for i = 1 to m $\\leftarrow (x^{(i)},y^{(i)})${<br>\n",
    "\\t set $a^{(1)} = x^{i}$<br>\n",
    "\\t preform forward propagation to compute $\\alpha^{l}$ for $l = 2,3,...,L$<br>\n",
    "\\t using $y{(i)}$, compute $\\delta{(L)} = \\alpha^{(L)}-y^{(i)}$ //$\\alpha$ is Hipotesis output AND $y^{(i)}$ what the traget layer was, __THUS__ the term is error the difference<br>\n",
    "\\t compute $\\delta^{(L-1)}, \\delta{(L-2)},...,\\delta{(2)}$ but not $\\delta^{(1)}$<br>\n",
    "\\t $\\color{red}{\\Delta_{ij}^{(l)}:=\\Delta_{ij}^{(l)}+\\alpha_j^{l}\\delta_i^{l+1}}$ $\\rightarrow$ __can be vectorzied__ $\\rightarrow$ $\\color{red}{\\Delta^{(l)}:=\\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T}$\n",
    "<br>} thus do not forget to enroll the matrices into vectors; it is easier to work with\n",
    "\n",
    "$D_{ij}^{l} := \\frac{1}{m}\\Delta_{ij}^{(l)}+\\lambda \\Theta_{ij}^{(l)} if j != 0$<br>\n",
    "$D_{ij}^{l} := \\frac{1}{m}\\Delta_{ij}^{(l)} if j == 0$\n",
    "<br><br>\n",
    "...__Thus__\n",
    "$$\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta)=D_{ij}^{(l)}$$\n",
    "<br>\n",
    "### Numerical estimation of gradients\n",
    "{pic: curvy line} where $\\epsilon = 10^{-4}$\n",
    "$$\\frac{d}{d\\theta} \\approx \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}$$\n",
    "$$\\therefore$$\n",
    "\n",
    "$$\\theta \\in \\mathbb{R}^n (\\text{e.g.} \\theta \\text{ is \"unrolled\" version of } \\Theta^{(1)},\\Theta^{(2)},\\Theta^{(3)} \\text{; where } \\theta = [\\theta_1,\\theta_2,\\theta_3,...,\\theta_n]$$\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\theta_1}J(\\theta) \\approx \\frac{\\theta_1+\\epsilon, \\theta_2, \\theta_3,...,\\theta_n - J(\\theta_1-\\epsilon,\\theta_2,\\theta_3,...,\\theta_n}{2\\epsilon}$$\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\theta_2}J(\\theta) \\approx \\frac{\\theta_1, \\theta_2+\\epsilon, \\theta_3,...,\\theta_n - J(\\theta_1,\\theta_2-\\epsilon,\\theta_3,...,\\theta_n}{2\\epsilon}$$\n",
    "\n",
    "$$...$$\n",
    "$$\\frac{\\delta}{\\delta\\theta_n}J(\\theta) \\approx \\frac{\\theta_1, \\theta_2, \\theta_3+\\epsilon,...,\\theta_n - J(\\theta_1,\\theta_2,...,\\theta_n-\\epsilon}{2\\epsilon}$$\n",
    "<br>\n",
    "__Implementation Note:__\n",
    "    - Impleement backprop to compute \"DVec\" (function to unroll Matrix in to vector)\n",
    "    - Implement numerical gradient check to compute \"gradApprox\"\n",
    "    - Make sure they give simliar values. (DVec $\\approx$ gradApprox)\n",
    "    - Turn off gradient checking. Using backprop code for learning.\n",
    "__Important:__\n",
    "    - Be sure to disable your gradient checking code before training your classifier. If you run numerical gradient computation on every iteration of gradient descent (or in the inner loop of \"costFuction(...))\" your code will be very slow.\n",
    "<br><br>\n",
    "\n",
    "### Random Initialization\n",
    "##### Zero initialization\n",
    "After each updata, parameters corresponding to inputs going into each of two hiden units are identical. THUS higly redunded, since it only guesse ONE feature; DO IT since the algorithm will learn more efficent\n",
    "##### Random initializaiton: Symmetry breaking\n",
    "Initialize each $\\Theta_{ij}^{(l)}$ to a random value in $[-\\epsilon, \\epsilon]$<br>\n",
    "(i.e. $-\\epsilon <= \\Theta_{ij}^{(l)} <= \\epsilon)$\n",
    "<br><br>\n",
    "\n",
    "### Training a neural network\n",
    "No. of input units: Dimension of features $x^{(i)}$<br>\n",
    "No. output units: Number of classes<br>\n",
    "Reasonable default: 1 hidden layer, of if > 1 hidden layer, have same no. of hiden units in every layer (usually the more the better)<br>\n",
    "##### Taxonmie\n",
    "1. Randomly initialze weights<br>\n",
    "2. Implement forward propagation to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$<br>\n",
    "3. Implement code to compute cost function $J(\\Theta)$<br>\n",
    "4. Implement backprop to compute partial derivatives $\\frac{\\delta}{\\delta\\Theta_{jk}^{l}}J(\\Theta)$<br>\n",
    "    for i = 1 to m{<br>\n",
    "    Preform forward propagation and backpropagation using example $(x^{(i)},y^{(i)})$<br>\n",
    "    (Get activations $\\alpha^{(l)}$ and delta terms $\\delta^{(l)}$ for $l = 2,...,L)$\n",
    "    $\\Delta^{(l)}:=\\Delta^{(l)}+\\delta^{l+1} (a^{(l)})^T$<br>\n",
    "    } compute: $\\frac{\\delta}{\\delta \\Theta_{jk}^{(l)}} J(\\Theta)$\n",
    "5. Use gradient checking to compare $\\frac{\\delta}{\\delta \\Theta_{jk}^{(l)}} J(\\Theta)$ compuuted using backpropagation vs. using numerical estimate of gradient of $J(\\Theta)$.<br>Then disable gradient checking code<br>\n",
    "6. Use gradient descent or advanced optimization method with backpropagation mean($\\frac{\\delta}{\\delta \\Theta_{jk}^{(l)}} J(\\Theta)$) to try to minimaize $J(\\Theta)$ as a funciton of parameters $\\Theta$\n",
    "<br><br>\n",
    "__NOTE:__ $J(\\Theta)$ Neural Network is __non-convex__ Optimzation\n",
    "<br>\n",
    "<img src=\"pic\\all_in_one.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
